{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play tic-tac-toe against various agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_tac_toe.agent import RandomAgent, SarsaAgent, QLearningAgent\n",
    "from tic_tac_toe.main import run_agent_against_human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "\n",
    "The random agent chooses uniformly among allowable next moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zufall = RandomAgent()\n",
    "run_agent_against_human(zufall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa agent\n",
    "\n",
    "Sarsa learning is an on-policy temporal difference control algorithm with update formula\n",
    "\n",
    "$$Q(s,a) = Q(s,a) + \\alpha * [\\mathrm{reward}(s') + \\gamma * Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "where $s'$ is the next state after taking action $a$ in state $s$ and $a'$ is the action to take by agent's policy (e.g. epsilon greedy)\n",
    "\n",
    "For more information, see\n",
    "* the [Wikipedia SARSA entry](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action)\n",
    "* Section 6.4 of [Reinforcement Learning, by Sutton and Barto](https://mitpress.mit.edu/books/reinforcement-learning-second-edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SarsaAgent()\n",
    "run_agend_against_human(sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning agent\n",
    "\n",
    "Q-learning is an off-policy temporal difference control algorithm with update formula\n",
    "\n",
    "$$Q(s,a) = Q(s,a) + \\alpha * [\\mathrm{reward}(s') + \\gamma * \\mathrm{max}(Q(s',a_)) - Q(s,a)]$$\n",
    "\n",
    "where $s'$ is the next state after taking action $a$ in state $s$ and $\\mathrm{max}(Q(s',a_))$ is the maximum of all the $Q$-values having state $s'$.\n",
    "\n",
    "For more information, see\n",
    "* the [Wikipedia Q-learning entry](https://en.wikipedia.org/wiki/Q-learning)\n",
    "* Section 6.7 of [Reinforcement Learning, by Sutton and Barto](https://mitpress.mit.edu/books/reinforcement-learning-second-edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearn = QLearningAgent()\n",
    "run_agent_against_human(qlearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
